---
layout: page
---
<h2 >Neural View Synthesis and Appearance Editing from Unstructured Images</h2>
<h4 class="text-success">Indian Conference on Computer Vision, Graphics and Image Processing (ICVGIP) 2021</h4>
<div >
  <span>
    <h6><a href="https://darthgera123.github.io/">Pulkit Gera<sup>1</sup></a>,
    <a href="https://aakashkt.github.io/">Aakash KT<sup>1</sup></a>,
    <a href="https://github.com/thesigmaguy">Dhawal Sirikonda<sup>1</sup></a>,
    <a href="https://scholar.google.co.in/citations?user=3HKjt_IAAAAJ&hl=en">P J Narayanan<sup>1</sup></a></h6>
  </span>
</div>

<h6><sup>1</sup><a href="https://cvit.iiit.ac.in/">CVIT, IIIT Hyderabad</a></h6>
<br>
<div class="btn-group" role="group" aria-label="Basic example">
    <a class = "text-black btn btn-outline-success" href="https://dl.acm.org/doi/abs/10.1145/3490035.3490299">Paper </a>
    <a class = "text-black btn btn-outline-success" href="https://github.com/darthgera123/Appearance-Editing">Code </a>
    </div>
<hr>
<img src="/assets/img/pub/app.png" width="900px" height="450px">
<hr>
<div>
	<h2 align="center" class="text-success"> Abstract</h2>
	<p>We present a neural rendering framework for simultaneous view synthesis and appearance editing of a scene from
  multi-view images captured under known environment illumination. Existing approaches either achieve view synthesis alone or view synthesis along with relighting, without direct control over the sceneâ€™s appearance. Our approach explicitly disentangles the appearance and learns a lighting representation that is independent of it. Specifically, we  independently estimate the BRDF and use it to learn a lighting-only representation of the scene. Such disentanglement allows our approach to generalize to arbitrary changes in appearance while performing view synthesis. We show results of editing the appearance of a real scene, demonstrating that our approach produces plausible appearance editing. The performance of our view synthesisapproach is demonstrated to be at par with state-of-the-art
  approaches on both real and synthetic data.</p>
 </div>

<hr>
<div>
	<h2 align="center" class="text-success"> Video</h2>
	<iframe width="800" height="400"src="https://www.youtube.com/embed/ZCVQj5FK0C4">
</iframe>
</div>
 <hr>
 <div>
	<h2 align="center" class="text-success">Pipeline</h2>
	<img src="/assets/img/pub/pipeline.png" width="800px" height="400px">
	<p>We propose a formulation that disentangles the Bi-directional Reflectance Distrubution Function (BRDF) and the captured object's local irradiance function (LIF). Both the BRDF and the LIF are estimated together, using a novel loss that ensures proper diffuse albedo estimation. This disentanglement allows the diffuse albedo to be edited or modified independently. The modified appearance is then combined with the learnt LIF to produce the final image for a novel viewpoint.</p>
 </div>
 <hr>
 <div>
	<h2 align="center" class="text-success">Results</h2>
	<img src="/assets/img/pub/results.png" width="800px" height="400px">
 </div>
 <hr>
 <div>
	<h2 align="center" class="text-success">Citation</h2>
	<pre>
	@inproceedings{10.1145/3490035.3490299,
	author = {Gera, Pulkit and T, Aakash K and Sirikonda, Dhawal and Narayanan, P. J.},
	title = {Neural View Synthesis with Appearance Editing from Unstructured Images},
	year = {2021},
	isbn = {9781450375962},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3490035.3490299},
	doi = {10.1145/3490035.3490299},
	booktitle = {Proceedings of the Twelfth Indian Conference on Computer Vision, Graphics and Image Processing},
	articleno = {40},
	numpages = {9},
	keywords = {text tagging, ACM proceedings, LATEX},
	location = {Jodhpur, India},
	series = {ICVGIP '21}
	}
	</pre>
 </div>

